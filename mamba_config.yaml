n_layers: 4             # Number of Mamba layers
d_model: 64             # Hidden dimension
input_dim: 224          # Size of input image dimension (e.g., 224x224)
dropout: 0.1            # Dropout rate
use_bias: true          # Whether to use bias in linear layers

# Optional fields â€” only include if your model expects them
activation: relu        # Activation function (e.g., relu, gelu, etc.)
norm_type: batchnorm    # Normalization layer
residual: true          # Whether to use residual connections
